{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5ca927a-7442-4173-87fc-aa954a58c221",
    "_uuid": "b315f4821bd700bf869a48f0838c15e18a89ea20",
    "colab_type": "text",
    "id": "axI4u1EkzGmT"
   },
   "source": [
    "# Convolutional Neural Networks with Data Augmentation using Keras\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## In this tutorial I will be using Keras with TensorFlow as backend to calssify digits from the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6de9fd1-9a50-4e58-a489-e69928b3c4b5",
    "_uuid": "96e2d0d769ff90a4d74a218c8fe61317518960be",
    "colab_type": "text",
    "id": "3icmSjgt1VLP"
   },
   "source": [
    "We will start by importing Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "eedc5d0b-2754-4615-b49a-b656afad0675",
    "_uuid": "77521748ee2a2edb717ad68b9f96bc5b841a5ebc",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "96_Dh7GYzFwv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "243ce545-305f-4a62-bb26-ab783972c70e",
    "_uuid": "804355b0c5d871f360a84b0ab704226ee7664214",
    "colab_type": "text",
    "id": "E5tzF9kr10GX"
   },
   "source": [
    "We then import the layers of the convolutional neural network.The network consists of two main components :\n",
    "\n",
    "1. Convolutional layers : the convolutional layer is responsible for the convolutional operation in which feature maps identifies features in the images.\n",
    "and is usually followed by two types of layers which are :\n",
    ">*   **Dropout** : Dropout is a regulization technique where you turn off part of the network's layers randomally to increase regulization and hense decrease overfitting. We use when the training set accuracy is muuch higher than the test set accuracy.\n",
    ">*   **Max Pooling** : The maximum output in a rectangular neighbourhood. It is used to make the network more flexible to slight changes and decrease the network computationl expenses by extracting the group of pixels that are highly contributing to each feature in the feature maps in the layer.\n",
    "2. Dense layers : The dense layer is a fully connected layer that comes after the convolutional layers and they give us the output vector of the Network.\n",
    "\n",
    "As a convention in Convolutional Neural Network we decrease the dimensions of the layers as we go deeper and increase the number of feature maps to make it detect more features and decrease the number of computational cost.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/MoghazyCoder/Machine-Learning-Tutorials/master/assets/Untitled.png)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "c615e843-f880-409a-9701-6ed334496e06",
    "_uuid": "a5e95affe023678faaa7e2c84797bb43fafa3fce",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EfQa_UJZ5wsR"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout,Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76d4c502-5b22-41c9-907d-9b96a45e31d7",
    "_uuid": "2fe3233cbe5b534701aa8d50fb5fe67c6691b7d8",
    "colab_type": "text",
    "id": "nlAMFOZO9feO"
   },
   "source": [
    "Sequential layers are stacked such that every layer passes its output to the next layer without you specifying extra information so we import Sequential from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "32118274-f023-4a7f-8d62-fe3a3796ff7b",
    "_uuid": "b6edf1f142a5195b829bc541bf42da023b5aeed4",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "T71utABW92J8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e952e8ad-e51c-4e2d-8163-6d86273f7f17",
    "_uuid": "27f3a5ae3ce5a87240cab3b9ba1f3f678830eb5a",
    "colab_type": "text",
    "id": "ja6xaTVOELGP"
   },
   "source": [
    "We must specify which data format convention Keras will follow using the following line of code. Keras can accept the number of channels before other dimensions or after it but here we have to specify which convention we will use. We will use channels last which is Tensorflow's convention ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "56fdb1f4-94be-4fdf-b072-03519b2b8da9",
    "_uuid": "b1a4dc85a2ee874f428d51cf736fe01c2de77a90",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_zASxWJTEJGr"
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "405477f2-49a2-4a82-933a-69f389a92ad8",
    "_uuid": "a4172fbcf2867bc9c3ce51ece2b79b26de64f894",
    "colab_type": "text",
    "id": "DBqW5EfhZ9q1"
   },
   "source": [
    "We should call mnist.load_data() which contains the mnist Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.  when we call mnist.load_data() it returns two tuples one for the training set containing the images and their corresponding lables and another one for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "898289f7-4acb-4343-8ef0-19aba44bfe8f",
    "_uuid": "66fbfe867582338a953b00b7fe2d7a8056d32818",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1701,
     "status": "ok",
     "timestamp": 1516128115363,
     "user": {
      "displayName": "Abd El Rhman ElMoghazy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112798608963931725749"
     },
     "user_tz": -120
    },
    "id": "hAEk70qRbUs8",
    "outputId": "674d2997-9754-4989-e278-3a515fb465bd"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('./input/02 - train.csv')\n",
    "test = pd.read_csv('./input/02 - test.csv')\n",
    "\n",
    "y = X[\"label\"]\n",
    "X.drop([\"label\"], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "e0e70d0813cd19bbee9022eb9a3ea51b8103d024"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2bcc3bcdedcdc1b50aa3582ecdb51a71af7e904f"
   },
   "source": [
    "We then reshape the samples according to TensorFlow convention which we chosed previously using \"K.set_image_data_format('channels_last')\" samples,rows,columns,channels as we are using channels_last if you are using channels_first you will need to change the order to samples,channels,rows,column and here we have only one channel because we are using the image in grayscale not RGB. We should also make the output in the form of one vs all (aka one hot encoding) which means that we will have 10 calsses from 0 to 9 one class for each number from 0 to 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e856a81b5c44ed2b8232f4125a4a34e7ba758314"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28 , 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28 , 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8e432a179ef833e69bb5158d96f16e4f990d38f"
   },
   "source": [
    "<h1>Data Exploration</h1>\n",
    "Let's explore the data we have as this will give us a hint on the algorithm we will use if we have to choose. Exploring data is also very important because it will tell you which accuracy metric you are going to use, if the data is balanced which means all the classes have fair contribution in the dataset regarding its numbers then we can easily use accuracy, But if the data is skewed then we won't be able to use accurace as it's results will be misleading and we may use F-beta score instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "8ee50b8d8d9187615aceb089d6af67cc89a8bb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of training examples = 33600\n",
      "the number of classes = 10\n",
      "Dimention of images = 28 x 28  \n",
      "The number of occuranc of each class in the dataset = {0: 3316, 1: 3775, 2: 3331, 3: 3414, 4: 3233, 5: 3093, 6: 3352, 7: 3508, 8: 3228, 9: 3350}  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"the number of training examples = %i\" % X_train.shape[0])\n",
    "print(\"the number of classes = %i\" % len(numpy.unique(y_train)))\n",
    "print(\"Dimention of images = {:d} x {:d}  \".format(X_train[1].shape[0],X_train[1].shape[1])  )\n",
    "\n",
    "#This line will allow us to know the number of occurrences of each specific class in the data\n",
    "unique, count= numpy.unique(y_train, return_counts=True)\n",
    "print(\"The number of occuranc of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )\n",
    " \n",
    "images_and_labels = list(zip(X_train,  y_train))\n",
    "for index, (image, label) in enumerate(images_and_labels[:12]):\n",
    "    plt.subplot(5, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('label: %i' % label )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d6c72777dffe414b82987d970eaf60a5b08c1a8"
   },
   "source": [
    "From the previous results we can see that the dataset consists of 60000 training example each is an image of dimention 28 * 28. We can see that the number of occurances of each class is almost balanced and based on that it is safe to use accuracy as our metric later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7bc85401-5205-4891-837e-ed87f809af10",
    "_uuid": "5a9a1f8ead473922206ecef2aeef24ea7837205e",
    "colab_type": "text",
    "id": "dr-O3FdEjZMG"
   },
   "source": [
    "<h1>  Model Design and Achitecture</h1>\n",
    "Now lets implement the first layer of the convolutional network as shown in the schema below and i will use a simple archicture simillar to LeCun's network a .\n",
    "![alt text](https://raw.githubusercontent.com/MoghazyCoder/Machine-Learning-Tutorials/master/assets/Layer.png)\n",
    "For the sequential model you just stack the layers and only specify the image input dimensions in the first layer.\n",
    "Our first layer will be a convolutional layer Conv2D() where we specify the number of feature maps , the input shape and the activation function which is here relu .The relu activation function is represented mathematically by max(0,X).\n",
    "We then add the max pooling layer (which is the most common kind of pooling) with a kernel of dimensions 2 * 2 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "4191bd33-3a05-44fb-9d04-230069de90b0",
    "_uuid": "32c0f10501334b47d509d1071ef4d43a10c4af03",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SHZ0G1fykEPM"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model.add(Conv2D(40, kernel_size=5, padding=\"same\",input_shape=(28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ec94ca1-9486-4a06-a93d-ed684c6f9200",
    "_uuid": "da8f3eb98b3db49f376611070f585941e934ecc2",
    "colab_type": "text",
    "id": "Jnn9RQ92lh7L"
   },
   "source": [
    "Lets add the 2nd layer but this time we increase the feature maps ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "feda7442-d644-43e6-93c6-82dbc5abc175",
    "_uuid": "b0a88c30c3064a9e57ac495c13caaa86603be155",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FCNLJgn5liic"
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(70, kernel_size=3, padding=\"same\", activation = 'relu'))\n",
    "model.add(Conv2D(500, kernel_size=3, padding=\"same\", activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(1024, kernel_size=3, padding=\"valid\", activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8195dc3b-a5c0-470b-8e91-57dd81f17940",
    "_uuid": "5dd8d45e683e99a8366b0a83a63e93b7ffa9a1fb",
    "colab_type": "text",
    "id": "7Zd7f3cel8A5"
   },
   "source": [
    "Now we add a flatten layer that takes the output of the CNN and flattens it and passes it as an input to the Dense Layers which passes it to the output layer.\n",
    "we have used number of classes = 10 because we have 10 numbers from 0 to 9 .\n",
    "every dense layer contains 300 neurons except for the output layer.\n",
    "We use Softmax with the output layer to output estimated probability vector for  multi-class classification ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "a50cea93-2f1c-4d80-a9df-e7e426037f2e",
    "_uuid": "7f430854d3a13b976815d7f0697a079e1b4c958f",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lieaM8XWl8kX"
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=100, activation='relu'  ))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=100, activation='relu'  ))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=100, activation='relu'  ))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e51b494-f2ad-4dc1-8d00-73c91004800e",
    "_uuid": "8bca856fd30e1e9e4871323a3d6e366f81e43c87",
    "colab_type": "text",
    "id": "l5wJGng0p6-B"
   },
   "source": [
    "We have to compile the model and then try training it using the fit() function which fits the training data and labels , the number of epochs and the batch_size which is the number of photos per training cycle.\n",
    "The last thing that we are going to do is to evaluate the model to ensure that it doesn't overfit the trainig data .Evaluating the model is done by using the weights that resulted from the training step and using it to estimate the value of the test data that the model haven't seen before to estimate how well the model will perform in the future on new data.\n",
    "\n",
    "if you are using cross-validation split then the convention is to split the data by 60% training set , 20% validation set and 20% test set but in the era of big data this ratio may vary according to the amount of data you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df82157c-45c5-414c-9aae-ba9670e5f1e4",
    "_uuid": "5edc4cb02f9d6f3f8ae7ae93d5299d49f4c8b935",
    "colab_type": "text",
    "id": "96_UW0g1hMDF"
   },
   "source": [
    "We have used categorical_crossentropy as the cost function for that model but what does we mean by **cost function**\n",
    "\n",
    "#### Cost function : It is a measure of the overall loss in our network after assigning values to the parameters during the forward phase so it indicates how well the parameters were chosen during the forward probagation phase.\n",
    "\n",
    "#### Optimizer : It is the gradiant descent algorithm that is used. We use it to minimize the cost function to approach the minimum point. We are using adam optimizer which is one of the best gradient descent algorithms. You can refere to this paper to know how it works https://arxiv.org/abs/1412.6980v8\n",
    "\n",
    "You can use other metrics to measure the performance other than accuracy as precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and     F1 score is a trade off between them. You can refere to this article for more about precision and recall http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "16f39f73d888f276d572b8e73fd39dafdb2d0a49"
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train).astype('int32')\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ed2cd184b2673e92e157f1f76bc2a38c5f679c5"
   },
   "source": [
    "I will use ImageDataGenerator from keras to augment the images. Augmenting the images makes the model more robust and more generalizable when using newly unseen data like the data in the test set of the competition. There are many ways to augment the images like centering the images, normalization, rotation, shifting, and flipping and i will use some of them [here](http://) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "812b3e24c4c5b9c767261cfdb50f2839e3f3e024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/12\n",
      "1050/1050 [==============================] - 23s 22ms/step - loss: 0.3065 - acc: 0.9040\n",
      "Epoch 2/12\n",
      "1050/1050 [==============================] - 21s 20ms/step - loss: 0.0952 - acc: 0.9748\n",
      "Epoch 3/12\n",
      "1050/1050 [==============================] - 23s 22ms/step - loss: 0.0708 - acc: 0.9809\n",
      "Epoch 4/12\n",
      "1050/1050 [==============================] - 23s 22ms/step - loss: 0.0638 - acc: 0.9834\n",
      "Epoch 5/12\n",
      "1050/1050 [==============================] - 27s 26ms/step - loss: 0.0572 - acc: 0.9851\n",
      "Epoch 6/12\n",
      "1050/1050 [==============================] - 22s 21ms/step - loss: 0.0567 - acc: 0.9866\n",
      "Epoch 7/12\n",
      "1050/1050 [==============================] - 22s 21ms/step - loss: 0.0483 - acc: 0.9874\n",
      "Epoch 8/12\n",
      "1050/1050 [==============================] - 22s 21ms/step - loss: 0.0395 - acc: 0.9899\n",
      "Epoch 9/12\n",
      "1050/1050 [==============================] - 22s 21ms/step - loss: 0.0398 - acc: 0.9899\n",
      "Epoch 10/12\n",
      "1050/1050 [==============================] - 21s 20ms/step - loss: 0.0404 - acc: 0.9900\n",
      "Epoch 11/12\n",
      "1050/1050 [==============================] - 22s 21ms/step - loss: 0.0377 - acc: 0.9902\n",
      "Epoch 12/12\n",
      "1050/1050 [==============================] - 22s 21ms/step - loss: 0.0359 - acc: 0.9910\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "X_train2 = numpy.array(X_train, copy=True) \n",
    "y_train2 = numpy.array(y_train, copy=True) \n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    )\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "print(type(X_train2))\n",
    "print(type(X_train))\n",
    "\n",
    "# Concatenating the old data with the augmented data\n",
    "result_x  = numpy.concatenate((X_train, X_train2), axis=0)\n",
    "result_y  = numpy.concatenate((y_train, y_train2), axis=0)\n",
    "\n",
    "\n",
    "# # fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(datagen.flow(result_x, result_y, batch_size=35),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "ff1d0906-5cdb-4bc7-b1cf-67bcc4266c80",
    "_uuid": "a578dfdf36a8734c102954e722d78251d4f0c36d",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 374,
     "output_extras": [
      {
       "item_id": 157
      },
      {
       "item_id": 286
      },
      {
       "item_id": 410
      },
      {
       "item_id": 534
      },
      {
       "item_id": 644
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 610614,
     "status": "ok",
     "timestamp": 1516130986715,
     "user": {
      "displayName": "Abd El Rhman ElMoghazy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112798608963931725749"
     },
     "user_tz": -120
    },
    "id": "XhwUKUarp80w",
    "outputId": "2c577b4a-6d5f-4077-defb-ae45fb7a5716",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16060572609864276, 0.9895238095238095]\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, y_train, epochs= 32 , batch_size=200, validation_split = 0.2)\n",
    "scores = model.evaluate(X_test, y_test, verbose = 10 )\n",
    "print ( scores )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "56c100a2800ed9b4973926206fe8e215e81c4729"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3\n",
       "5        6      7\n",
       "6        7      0\n",
       "7        8      3\n",
       "8        9      0\n",
       "9       10      3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = (test.values).reshape(-1, 28, 28 , 1).astype('float32')\n",
    "\n",
    "res = model.predict(test_set)\n",
    "res = numpy.argmax(res,axis = 1)\n",
    "res = pd.Series(res, name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1 ,28001) ,name = \"ImageId\"),   res],axis = 1)\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "36f8febd-6203-4551-abb4-36964a434417",
    "_uuid": "2fb0407cc6ae233656c13198a6db319a119768b5"
   },
   "source": [
    "Note that these results can be further optimized and regulized but i will leave that for you. Knowing how to optimize the results is very important and can be done using error analysis techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ad354ed9-e997-4048-ba23-52858b34d74c",
    "_uuid": "8f692682b13b39316b9de77c41f4f2cbaf4cf7f3",
    "colab_type": "text",
    "id": "NYNAQuZxtHIQ"
   },
   "source": [
    "### This tutorial is written by AbdElRhman ElMoghazy.\n",
    "\n",
    "### Refrences ,Textbooks and Tutorials :\n",
    "Hands on machine learning with scikit-learn and TensorFlow by Aurélien Géron\n",
    "\n",
    "Pyhron machine learning 2nd edition by Sebastian Raschka ,Vahid Mirjalili\n",
    "\n",
    "http://www.deeplearningbook.org/\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "\n",
    "https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/index.html?index=..%2F..%2Findex#0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Convolutional Neural Networks.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
